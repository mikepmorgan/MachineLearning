---
title: "Machine Learning Final"
author: "Michael Morgan"
date: "March 26, 2016"
output: html_document
---

Loading the libraries we'll need for our analysis: 

```{r setup, include=TRUE}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(dplyr)
```

First we will download and load the relevant files. Please note that this data has been generously provided for public use. See citation.[^1]


```{r, cache=TRUE}
trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainurl, "traindata.csv")
download.file(testurl, "testdata.csv")
testdat <- read.csv("testdata.csv", stringsAsFactors = FALSE)
traindat <- read.csv("traindata.csv", stringsAsFactors = FALSE)
```

##Cross Validation & Data Slicing 

First I will set a seed to keep the analysis traceable, and will partition the data into my own training and testing sets. I am doing this so I can first analyze the models on a sub-test set, and keep the final testing set as a validation set. 

```{r pressure, echo=TRUE, cache=TRUE}
set.seed(12345)
mysamps <- createDataPartition(traindat$classe, p = 0.75, list=FALSE)
mytrain <- traindat[mysamps,]
mytest <- traindat[-mysamps,]
```

Some of the variables within the data set appear to be irrelavant to the prediction, and thus I am removing them from the start of the analsysis. These include in the participant name, ID, and the timestamps when the exercise occured. These are in the first seven columns, and I will simply subset them out.

```{r, cache=TRUE, include=TRUE}
mytrain <- mytrain[,8:160]
mytest <- mytest[,8:160]
testdat <- testdat[,8:160]
```

Using the View function in dplyr, I can see that there are a large number of NA datapoints within the loaded data set. Using sapply and dplyr's mutate function, I can see that there are some variables with a very high proportion of NA values. 

```{r, cache=TRUE, include=TRUE}
na_count <- sapply(mytrain, function(y) sum(length(which(is.na(y)))))
na_count <- as.data.frame(na_count)
na_df <- mutate(na_count, proportion_na = na_count/14718)

```

Displaying the dataframe is an eyesore (see the HTML file if you prefer) but it does help to see just how many variables have about 98% of their observations as NA. 

As in the lectures, I've decided to use the near zero function in caret to remove these features, as applicable, if they have limited value for purposes of model building. dplyr will allow me to drop those fields which have limited analytical impacts from my training and test sets. I am also dropping them from the final test set. After that, I am changing the remaining NA fields to zero in all of the data sets. 

```{r, cache=TRUE, include=TRUE}
nsv <- nearZeroVar(mytrain, names = TRUE)

mytrain <- select(mytrain, -one_of(nsv))
mytest <- select(mytest, -one_of(nsv))
testdat <- select(testdat, -one_of(nsv))



mytrain[is.na(mytrain)] <- 0
mytest[is.na(mytest)] <- 0
testdat[is.na(testdat)] <- 0

mytrain$classe <- as.factor(mytrain$classe)
mytest$classe <- as.factor(mytest$classe)

```


## Model Building 

I will use two models to assess the data, Random Forests and predicting with Trees. First, let's take a look at the variables we're seeking to predict. 

```{r}
plot(table(mytrain$classe))
```
 
##Random Forest Model 
 
``` {r, cache=TRUE}
 randfor <- randomForest(classe~., data = mytrain, method = "class")
        predforest <- predict(randfor, mytest, type = "class")
        confusionMatrix(predforest, mytest$classe)
```


The accuracy of the random forest model is .9943, which means the out of sample error that I anticipate is approximately .0057 -- very low. 


##Tree Model 

```{r, include=TRUE}
treemod <- train(classe ~ ., method = "rpart", data = mytrain)
print(treemod$finalModel)
plot(treemod$finalModel, uniform = TRUE, main = "Classification Tree")
text(treemod$finalModel, use.n=TRUE, all=TRUE, cex=.8)

predtree <- predict(treemod, newdata = mytest)
confusionMatrix(predtree, mytest$classe)

```


The accuracy of the tree model is .54, which means I would anticipate a fairly large out of sample error of .46. This mdoel has significantly more error than the Random Forest model. 

###Selection of Final Model 

Based on the accuracies described in the Confusion Matrices, I have chosen the Random Forest model as the most appropriate model for the final test. The accuracy is clearly superior, and while the code can somtimes take a significant amount of time to run, it is not overly burdensome for our current data set. 


```{r, cache=TRUE, echo=TRUE, include=TRUE}
answers <- predict(randfor, testdat, type = "class")
answers 
```


The outputed answers, which I will submit for the final, are: 

1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 

B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 

[^1]: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.


